{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"date_time\", DateType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"table_state\", StringType(), True),\n",
    "    StructField(\"rows_inserted\", IntegerType(), True),\n",
    "    StructField(\"metric_value\", FloatType(), True),\n",
    "    StructField(\"metric_name\", StringType(), True),\n",
    "    StructField(\"metric_config\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "table_state = dbutils.widgets.get(\"table_state\")\n",
    "rows_inserted = dbutils.widgets.get(\"rows_inserted\")\n",
    "last_update_time = dbutils.widgets.get(\"last_update_time\")\n",
    "last_update_time = datetime.fromisoformat(last_update_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ingestion_delay(table_name, end_time):\n",
    "    try:\n",
    "        timestamp_ingestion = spark.sql(f\"select max(timestamp_ingestion) from prod.rwd.{table_name}\").collect()[0][0]\n",
    "    except:\n",
    "        timestamp_ingestion = None\n",
    "\n",
    "    if timestamp_ingestion is not None:\n",
    "        timestamp_ingestion = timestamp_ingestion.replace(tzinfo=None)\n",
    "    if end_time is not None:\n",
    "        end_time = end_time.replace(tzinfo=None)\n",
    "\n",
    "    if timestamp_ingestion is None or end_time is None:\n",
    "        return None, {\"timestamp_ingestion\": timestamp_ingestion, \"end_time\": end_time}\n",
    "    \n",
    "    return float(abs(end_time - timestamp_ingestion).seconds / 60), {\"timestamp_ingestion\": timestamp_ingestion, \"end_time\": end_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now(pytz.timezone(\"America/Sao_Paulo\")).replace(tzinfo=None)\n",
    "metric_value, metric_config = calculate_ingestion_delay(table_name, last_update_time)\n",
    "\n",
    "obj_to_save = {\n",
    "    \"table_name\": table_name,\n",
    "    \"date_time\": date.today(),\n",
    "    \"start_time\": start_time,\n",
    "    \"end_time\": datetime.now(pytz.timezone(\"America/Sao_Paulo\")).replace(tzinfo=None),\n",
    "    \"table_state\": table_state,\n",
    "    \"rows_inserted\": int(float(rows_inserted)),\n",
    "    \"metric_value\": metric_value,\n",
    "    \"metric_name\": \"ingestion_delay\",\n",
    "    \"metric_config\": metric_config\n",
    "}\n",
    "\n",
    "objs = Row(**obj_to_save)\n",
    "df = spark.createDataFrame([objs], schema=schema)\n",
    "path = f\"s3://recargapay-databricks-prod/timeliness_metrics_teste/{date.today().isoformat()}\"\n",
    "df.write.parquet(path, mode=\"append\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
